<!DOCTYPE html>
<html lang="en">

  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    
    <meta name="description" content="My Website">
    <meta name="keywords" content="blog,developer,AI,ML,BI,DataScience">

    <meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Various Concepts Around Linear Regression"/>
<meta name="twitter:description" content="Various Concepts Around Linear Regression Interpreting Regression Output Slope $(\beta_i)$ / Intercept-$(\beta_0)$ Estimated chage in the average value of Y resulting from 1-unit change in the input feature $X_i.$ Whereas intercept term measured the average Value of Y When X = 0.
$R^2$ R-square explains what proprtion of the total variation is explained by the regression equation, which obviously implies that higher R-square values indicates better model.
Adjusted $R^2$ Since R square will continue increasing with each additional regressor (x variable) coming in, it tends to overfit the data."/>

    <meta property="og:title" content="Various Concepts Around Linear Regression" />
<meta property="og:description" content="Various Concepts Around Linear Regression Interpreting Regression Output Slope $(\beta_i)$ / Intercept-$(\beta_0)$ Estimated chage in the average value of Y resulting from 1-unit change in the input feature $X_i.$ Whereas intercept term measured the average Value of Y When X = 0.
$R^2$ R-square explains what proprtion of the total variation is explained by the regression equation, which obviously implies that higher R-square values indicates better model.
Adjusted $R^2$ Since R square will continue increasing with each additional regressor (x variable) coming in, it tends to overfit the data." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://skubidu.github.io/blog/art004/" />
<meta property="article:published_time" content="2019-12-11T14:43:13+01:00" />
<meta property="article:modified_time" content="2019-12-11T14:43:13+01:00" />


    
      <base href="https://skubidu.github.io/blog/art004/">
    
    <title>
  Various Concepts Around Linear Regression Â· My Website
</title>

    
      <link rel="canonical" href="https://skubidu.github.io/blog/art004/">
    

    <link href="https://fonts.googleapis.com/css?family=Lato:400,700%7CMerriweather:300,700%7CSource+Code+Pro:400,700" rel="stylesheet">
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.11.2/css/all.css" integrity="sha384-KA6wR/X5RY4zFAHpv/CnoG2UW1uogYfdnP67Uv7eULvTveboZJg0qUpmJZb5VqzN" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/normalize/8.0.1/normalize.min.css" integrity="sha256-l85OmPOjvil/SOvVt3HnSSjzF1TUMyT9eV0c2BzEGzU=" crossorigin="anonymous" />

    
      
      
      <link rel="stylesheet" href="https://skubidu.github.io/css/coder.min.28d751104f30c16da1aa1bb04015cbe662cacfe0d1b01af4f2240ad58580069c.css" integrity="sha256-KNdREE8wwW2hqhuwQBXL5mLKz&#43;DRsBr08iQK1YWABpw=" crossorigin="anonymous" media="screen" />
    

    

    

    

    
    
    <link rel="icon" type="image/png" href="https://skubidu.github.io/images/favicon-32x32.png" sizes="32x32">
    <link rel="icon" type="image/png" href="https://skubidu.github.io/images/favicon-16x16.png" sizes="16x16">

    <meta name="generator" content="Hugo 0.60.1" />
  </head>

  
  
  <body class="colorscheme-light">
    <main class="wrapper">
      <nav class="navigation">
  <section class="container">
    <a class="navigation-title" href="https://skubidu.github.io/">
      My Website
    </a>
    <input type="checkbox" id="menu-toggle" />
    <label class="menu-button float-right" for="menu-toggle"><i class="fas fa-bars"></i></label>
    <ul class="navigation-list">
      
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://skubidu.github.io/about/">About</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://skubidu.github.io/blog/">Blog</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://skubidu.github.io/projects/">Projects</a>
          </li>
        
          <li class="navigation-item">
            <a class="navigation-link" href="https://skubidu.github.io/contact/">Contact me</a>
          </li>
        
      
      
    </ul>
  </section>
</nav>


      <div class="content">
        
  <section class="container page">
  <article>
    <header>
      <h1>Various Concepts Around Linear Regression</h1>
    </header>

    <h1 id="various-concepts-around-linear-regression">Various Concepts Around Linear Regression</h1>
<h1 id="interpreting-regression-output">Interpreting Regression Output</h1>
<h4 id="slope-betai--interceptbeta0">Slope $(\beta_i)$ / Intercept-$(\beta_0)$</h4>
<p>Estimated chage in the average value of Y resulting from 1-unit change in the input feature $X_i.$ Whereas intercept term measured the average Value of Y When X = 0.</p>
<h4 id="r2">$R^2$</h4>
<p>R-square explains what proprtion of the total variation is explained by the regression equation, which obviously implies that higher R-square values indicates better model.</p>
<h4 id="adjusted-r2">Adjusted $R^2$</h4>
<p>Since R square will continue increasing with each additional regressor (x variable) coming in, it tends to overfit the data. Thus, the concept of adj. R-square was derived, which penalizes unnecessary increasing of variables, and does not necessarily increase with the increase in no. of regressors.</p>
<h4 id="tstatistic">t-statistic</h4>
<p>The coefficient t-value is a measure of how many standard deviations our coefficient estimate is far away from 0. We want it be far away from zero as this would indicate we could reject the null hypothesis - that is, we could decleare a relationship between x and y exist.</p>
<h4 id="fstat">F-Stat</h4>
<p>F-Stat is a good indicator of wheather there is a relationship between our predictor and the response variables. The further the F-statistic is from 1 the better it is. However how much larger the F-statistic needs to be depends on both the number of data points and the number of predictors.</p>
<h4 id="residual-se">Residual SE</h4>
<p>Residual Standard Error is measure of the quality of a linear regression fit. Theoretically, every linear model is assumed to contain an error term. The Residual Standard Error is the average amount that the response will deviate from the true regression line.</p>
<h4 id="tolerance">Tolerance</h4>
<p>In multiple regression, tolerance is used aas an indicator of multicollinearity. As a point of interest, tolerance may be said to be the opposite of the coefficent of determination. In that sense, tolerance is identical to the coefficient of alienation.</p>
<h4 id="vif">VIF</h4>
<p>In multiple regression, the variance inflation factor (VIF) is used as an indicator of multicollinearity. Computationally, it is defines as the reciprocal of tolerance: $\frac{1}{1-R^2}$. All other things equal, researchers desire lower levels of VIF.</p>
<h4 id="dw-statistic">DW Statistic</h4>
<p>It is a test statistic used to detect the presence of autocorrelation at lag 1 in the residuals form a regression analysis.</p>
<h4 id="breusch-pagan-test">Breusch Pagan Test</h4>
<p>It is used to test for heteroscedasticity in a linear regression model. It is a chi-squared test, where if the test statistic has a p-value below an appopriate threshold (e.g. p&lt;0.05) then the null hypothesis of homoscedaticity is rejected.</p>
<h4 id="boxcoxtransformation">Box-Cox-Transformation</h4>
<p>Also known as the power-normal distribution. It is the distribution of a random variable X for which the Box-Cox tranformation on X follows a truncated normal distribution. It is a continuous probability distribution having probability density function. Often Box-Cox-Transformation solves the problem of non-linearity in other function of forms.</p>
<h1 id="how-to-to-deal-with-overfitting--linear-regression">How to to Deal with Overfitting - Linear Regression?</h1>
<ul>
<li>Regularization refers to the process of making the model less complex which can further allow it to gereralize better (i.e. avoid overfitting) and perform better on new data.</li>
</ul>
<h2 id="ridge-regression">Ridge Regression</h2>
<figure>
    <img src="https://skubidu.github.io/images/RidgeReg.jpg"
         alt="Geometric Repressentation of Ridge Regression" width="400" height="400"/> <figcaption>
            <p>Geometric Repressentation of Ridge Regression</p>
        </figcaption>
</figure>

<h2 id="lasso-regression">Lasso Regression</h2>
<figure>
    <img src="https://skubidu.github.io/images/LassoReg.jpg"
         alt="Geometric Repressentation of Lasso Regression" width="400" height="400"/> <figcaption>
            <p>Geometric Repressentation of Lasso Regression</p>
        </figcaption>
</figure>

<ul>
<li><strong>Choice of techniques:</strong> So when sould you use Linear Regression, Ridge, Lasso or Elastic Net?
<ul>
<li>It is almost always required to have at least a little bit of regularizatio, so generally one should avoid plain Linear Regression.</li>
<li>Ridge is a good default, but in presence of many redundant features, Lasso or Elastic Net should be the first choidce as they tend to reduce the useless fueatures weights down to zero.</li>
<li>In general, Elastic Net is preferred over Lasso since Lasso may be a bit unstable when the number of features is greater than the number of trainig instances or in presence of very high multi-collinearity</li>
</ul>
</li>
</ul>
<h2 id="performance-measures--lineare-regression">Performance Measures - Lineare Regression</h2>
<h4 id="press-statistic">PRESS Statistic</h4>
<p>In statistics, the predicted residual error sum of squares (PRESS) statistic is a form of cross-validation used in regression analysis to provide a summary measure of the fit of model.</p>
<h4 id="mse">MSE</h4>
<p>Average squared deviation of the predicted outcome from the actual outcome.
$$
MSE = \frac{1}{n} \sum_{i=1}^{n}(Y_i - \hat{Y}_i)^2
$$</p>
<h4 id="rmse">RMSE</h4>
<p>(RMSE) is the square root of the mean of the squared errors. RMSE it's better for showing bigger deviations,
$$
MSE = \sqrt{ \frac{1}{n} \sum_{i=1}^{n}(Y_i - \hat{Y}_i)^2 }
$$</p>
<h4 id="mae">MAE</h4>
<p>(MAE) is the mean of the absolute value of the errors. It is less sesitive to the occasional very large error because it does not square the errors in the calculation
$$
MAE =  \frac{1}{n} \sum_{i=1}^{n} |Y_i - \hat{Y}_i|
$$</p>
<h4 id="mape">MAPE</h4>
<p>MAPE is the Mean absolute percentage error and it measures the size of the error in percentage terms. MAPE is useful to compare the precision between different volumes uder study.</p>
<p>$$
MAPE = \frac{100}{n} \sum_{i=1}^{n} \bigg| \frac{Y_i - \hat{Y}_i}{Y_i}\bigg|
$$</p>
<h4 id="measures--model-comparision">Measure(s) - model comparision:</h4>
<ul>
<li>
<p>AIC</p>
<ul>
<li>The magnitude of AIC (Akaike information Criteria) for specific model is less of intrest than the compariosn of AICs for two or more models because AIC is usally used in context of comparing models, not as an absolute criterion in itself.</li>
<li>The usual <em><strong>rule</strong></em> for comparing two or more models is to choose the one with the minimum AIC value.</li>
</ul>
</li>
<li>
<p>BIC/SIC</p>
<ul>
<li>Im statistics, the Bayesian Information Criterion (BIC) or Schwarz Criterion (also SBC, SBIC) is a criterion for model selection among finte set of model, the modet with the lowest BIC is prefferd</li>
<li>It is based, in part, on the likelihood function and its is closely related to the Akaike Information Criterion (AIC).</li>
</ul>
</li>
</ul>

  </article>
</section>


      </div>

      <footer class="footer">
  <section class="container">
    
    
    
    
    
 <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.css">

 <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/katex.min.js"></script>

 <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.7.1/contrib/auto-render.min.js"></script>

 <script>
renderMathInElement(
	document.body,
	{
    	delimiters: [
        	{left: "$$", right: "$$", display: true},
        	{left: "\\[", right: "\\]", display: true},
        	{left: "$", right: "$", display: false},
        	{left: "\\(", right: "\\)", display: false}
        ]
	}
);
</script>


  </section>
</footer>

    </main>

    

  </body>

</html>
